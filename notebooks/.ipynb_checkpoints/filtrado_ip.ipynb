{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrado de tablas para obtener información de actividades solo de la asignatura IP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este script es el de filtrar las tablas que recogen información de las actividades llevadas a cabo por los usuarios para que alberguen información relativa solo al curso de ip, y así evitar tener que hacer un join para concatenar usuario con la tarea/foro/cuestionario y otro para unir estas tuplas con aquellas actividades pertenecientes a un curso. \n",
    "\n",
    "Además, una vez hecho esto, podemos concatenar en algunos casos información de dos tablas en una sola para reducir la complejidad de las consultas y mejorar la eficiencia de cómputo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/05/26 17:29:04 WARN Utils: Your hostname, carlos-Modern-15-A11SB, resolves to a loopback address: 127.0.1.1; using 158.49.195.162 instead (on interface wlo1)\n",
      "25/05/26 17:29:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/26 17:29:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "\n",
    "# Configuración\n",
    "curso_ip = 8683\n",
    "ruta_origen = \"/home/carlos/Documentos/TFG/spark-workspace/data/raw\"\n",
    "ruta_destino = \"/home/carlos/Documentos/TFG/spark-workspace/data/raw/ip\"\n",
    "os.makedirs(ruta_destino, exist_ok=True)\n",
    "\n",
    "# Crear sesión Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Filtrado datos curso IP\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de assignments y concatenación de entregas y notas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras realizar esta operación, nos quedaremos con una única tabla  `assign_submission_grade_cmi` que va a tener información de  todas las tareas entregadas por los estudiantes de ip, junto con la calificación que obtuvieron en ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assign_submission_grade_cmi.parquet creado :)\n"
     ]
    }
   ],
   "source": [
    "# Leer datos de asignaciones y envíos\n",
    "assign = spark.read.parquet(f\"{ruta_origen}/assign_cmi.parquet\")\n",
    "submissions = spark.read.parquet(f\"{ruta_origen}/assign_submission_cmi.parquet\")\n",
    "\n",
    "# Filtrar tabla para quedarnos solo con las tareas de ip , y quedarnos solo con los campos necesarios\n",
    "assign_ip = assign.filter(col(\"course\") == curso_ip).select(\n",
    "    \"id\", \"duedate\", \"allowsubmissionsfromdate\", \"name\"\n",
    ")\n",
    "# Obtener entregas de tareas del curso IP\n",
    "submissions_ip = (\n",
    "    submissions.join(assign_ip, submissions.assignment == assign_ip.id, \"inner\")\n",
    "    .withColumnRenamed(\"timemodified\", \"timesubmitted\")\n",
    "    .drop(\"id\")\n",
    ")\n",
    "\n",
    "# Concatenar submissions con grades\n",
    "grades = spark.read.parquet(f\"{ruta_origen}/assign_grades_cmi.parquet\")\n",
    "grades_filtered = grades.select(\"userid\", \"assignment\", \"grade\")\n",
    "\n",
    "assign_submission_grade_cmi = submissions_ip.join(grades_filtered, on=[\"userid\", \"assignment\"], how=\"left\")\n",
    "\n",
    "# Escribir el dataframe a parquet para dejarlo listo para métricas\n",
    "if not os.path.exists(f\"{ruta_destino}/assign_submission_cmi.parquet\"):\n",
    "    assign_submission_grade_cmi.write.mode(\"overwrite\").parquet(f\"{ruta_destino}/assign_submission_grade_cmi.parquet\")\n",
    "    print(\"assign_submission_grade_cmi.parquet creado :)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de mensajes en los foros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado de este filtrado será obtener una única tabla `forum_post_cmi` en la que cada tupla tendrá la información de cada mensaje que ha publicado un alumno en cualquiera de los foros de la asignatura ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " forum_posts_ip.parquet creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cargar datos\n",
    "forum = spark.read.parquet(f\"{ruta_origen}/forum_cmi.parquet\")\n",
    "discussions = spark.read.parquet(f\"{ruta_origen}/forum_discussions_cmi.parquet\")\n",
    "posts = spark.read.parquet(f\"{ruta_origen}/forum_posts_cmi.parquet\")\n",
    "\n",
    "\n",
    "\n",
    "# Filtrar foros del curso IP\n",
    "forum_ip = forum.filter(col(\"course\") == curso_ip).select(\"id\")  # id = forum_id\n",
    "\n",
    "# Unir discussions con forum_ip para quedarnos con discusiones de IP\n",
    "discussions_ip = discussions.join(forum_ip, discussions.forum == forum_ip.id, \"inner\") \\\n",
    "                            .select(discussions[\"id\"].alias(\"discussion_id\"))\n",
    "\n",
    "# Unir posts con discussions_ip para quedarnos solo con los posts válidos\n",
    "posts_ip = posts.join(discussions_ip, posts.discussion == discussions_ip.discussion_id, \"inner\") \\\n",
    "                .select(\"userid\", \"discussion\", \"created\")\n",
    "                \n",
    "# Guardar parquet\n",
    "posts_ip.write.mode(\"overwrite\").parquet(f\"{ruta_destino}/forum_posts_ip.parquet\")\n",
    "print(\" forum_posts_ip.parquet creado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de cuestionarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- course: long (nullable = true)\n",
      " |-- timeopen: long (nullable = true)\n",
      " |-- timeclose: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- timemodified: long (nullable = true)\n",
      "\n",
      "========================================\n",
      "root\n",
      " |-- quiz: long (nullable = true)\n",
      " |-- userid: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- attempt: long (nullable = true)\n",
      " |-- sumgrades: string (nullable = true)\n",
      " |-- timestart: long (nullable = true)\n",
      " |-- timefinish: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cargar datos\n",
    "quizes = spark.read.parquet(f\"{ruta_origen}/quiz_cmi.parquet\")\n",
    "attempts = spark.read.parquet(f\"{ruta_origen}/quiz_attempts_cmi.parquet\")\n",
    "\n",
    "quizes.printSchema()\n",
    "print(\"========================================\")\n",
    "attempts.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark (pyspark-env)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
