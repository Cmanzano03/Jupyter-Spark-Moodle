{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Union de df de actividades estandar en uno solo\n",
    "---\n",
    "En este notebook vamos a unir todos los dataframe estandar en uno solo que reuna la información de todas las actividades. Antes de juntarlos en un solo dataframe, debemos establecer la ventana de tiempo que vamos a considerar para recoger las features de actividad, y explorar si en los vpl y los cuestionarios hay calificaciones -1, para explorar si las marcamos como no entregadas o como entregadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, concat_ws, count, avg, coalesce, to_date, from_unixtime, max\n",
    "\n",
    "#  Crear sesión Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EvaluacionContinuaIP\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#  Rutas base\n",
    "PATH_RAW = \"/home/carlos/Documentos/TFG/spark-workspace/data/raw\"\n",
    "PATH_INTERMEDIATE_CON = \"/home/carlos/Documentos/TFG/spark-workspace/data/intermediate/concatenados\"\n",
    "PATH_INTERMEDIATE_STD = \"/home/carlos/Documentos/TFG/spark-workspace/data/intermediate/estandarizados\"\n",
    "\n",
    "PATH_RESULTADOS = \"/home/cmanzanoo/Documentos/salida_notebook_evaluacion_ip\"\n",
    "\n",
    "# Crear carpeta de resultados si no existe (opcional en local)\n",
    "import os\n",
    "os.makedirs(PATH_RESULTADOS, exist_ok=True)\n",
    "\n",
    "# Verificación\n",
    "print(\"✔️ Spark configurado y rutas preparadas.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
